{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c8bfdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "import tiktoken\n",
    "from torch.utils.data import DataLoader,Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d78e9c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 256, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "519f5457",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        \n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        \n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3266e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7a30986",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f4ba327",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "39f902df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c2a1c627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c969fff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model,idx,max_tokens,context_size):\n",
    "    for i in range(max_tokens):\n",
    "        idx_trimmed = idx[::,-context_size:]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "                logits = model(idx_trimmed)\n",
    "        \n",
    "        logit = logits[:,-1,:]\n",
    "        \n",
    "        logit_prob = torch.softmax(logit,dim=-1)\n",
    "        \n",
    "        idx_next = torch.argmax(logit_prob,dim=-1,keepdim=True)\n",
    "        \n",
    "        idx = torch.cat((idx,idx_next),dim=1)\n",
    "        \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1375734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ccbebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond) ### batch, n_tokens, vocab_size\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  \n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3628cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    input_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1eed9471",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "split_id = int(train_ratio*len(input_text))\n",
    "train_data = input_text[:split_id]\n",
    "val_data = input_text[split_id:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "629d1935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(input_text))\n",
    "train_data = input_text[:split_idx]\n",
    "val_data = input_text[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "727198aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "total_characters = len(input_text)\n",
    "total_tokens = len(tokenizer.encode(input_text))\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "    \n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "59f497a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "9\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "89e1f17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7b301838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d5f1c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader,model,num_batches,device):\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches,len(data_loader))\n",
    "    for i,(input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i<num_batches:\n",
    "            loss = calc_loss_batch(input_batch,target_batch,model,device)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss/num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b7e3763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(train_loader,val_loader,model,eval_freq,device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader,model,eval_freq,device)\n",
    "        val_loss = calc_loss_loader(val_loader,model,eval_freq,device)\n",
    "    model.train()\n",
    "    return train_loss,val_loss\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5135d67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e440701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ef6a9fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch,target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch,target_batch,model,device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen+= input_batch.numel() # total number of elements seen upto to current batch\n",
    "            global_step+=1\n",
    "            \n",
    "            if global_step%eval_freq==0: # display status every _ number of batches\n",
    "                train_loss, val_loss = evaluate_model(train_loader,val_loader,model,eval_freq,device)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Epoch {epoch+1} (Step {global_step}): Train Loss {train_loss:.3f} Val Loss {val_loss:.3f}\")\n",
    "            \n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e3a8b0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Step 0): Train Loss 9.781 Val Loss 9.933\n",
      "Epoch 1 (Step 5): Train Loss 8.111 Val Loss 8.339\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Epoch 2 (Step 10): Train Loss 6.661 Val Loss 7.048\n",
      "Epoch 2 (Step 15): Train Loss 5.961 Val Loss 6.616\n",
      "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,, and, and,\n",
      "Epoch 3 (Step 20): Train Loss 5.726 Val Loss 6.600\n",
      "Epoch 3 (Step 25): Train Loss 5.201 Val Loss 6.348\n",
      "Every effort moves you, and I had been.                                            \n",
      "Epoch 4 (Step 30): Train Loss 4.417 Val Loss 6.278\n",
      "Epoch 4 (Step 35): Train Loss 4.069 Val Loss 6.226\n",
      "Every effort moves you know the                          \"I he had the donkey and I had the and I had the donkey and down the room, I had\n",
      "Epoch 5 (Step 40): Train Loss 3.732 Val Loss 6.160\n",
      "Every effort moves you know it was not that the picture--I had the fact by the last I had been--his, and in the            \"Oh, and he said, and down the room, and in\n",
      "Epoch 6 (Step 45): Train Loss 2.850 Val Loss 6.179\n",
      "Epoch 6 (Step 50): Train Loss 2.427 Val Loss 6.141\n",
      "Every effort moves you know,\" was one of the picture. The--I had a little of a little: \"Yes, and in fact, and in the picture was, and I had been at my elbow and as his pictures, and down the room, I had\n",
      "Epoch 7 (Step 55): Train Loss 2.104 Val Loss 6.134\n",
      "Epoch 7 (Step 60): Train Loss 1.882 Val Loss 6.233\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I was no--as! The women had been, in the moment--as Jack himself, as once one had been the donkey, and were, and in his\n",
      "Epoch 8 (Step 65): Train Loss 1.320 Val Loss 6.238\n",
      "Epoch 8 (Step 70): Train Loss 0.985 Val Loss 6.242\n",
      "Every effort moves you know,\" was one of the axioms he had been the tips of a self-confident moustache, I felt to see a smile behind his close grayish beard--as if he had the donkey. \"strongest,\" as his\n",
      "Epoch 9 (Step 75): Train Loss 0.717 Val Loss 6.293\n",
      "Epoch 9 (Step 80): Train Loss 0.541 Val Loss 6.393\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the window-curtains, I had the donkey. \"There were days when I\n",
      "Epoch 10 (Step 85): Train Loss 0.391 Val Loss 6.452\n",
      "Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gis\n",
      "Training completed in 5.31 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Note:\n",
    "# Uncomment the following code to calculate the execution time\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Note:\n",
    "# Uncomment the following code to show the execution time\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "98968b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0877a2cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_loss(epochs_seen, tokens_seen, train_losses,val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5,3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.png\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_loss(epochs_tensor, tokens_seen, train_losses, val_losses)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "66e3f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5622da4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you ever without bitterness, and pushed one of the deep arm-chairs forward.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=10,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "29b5567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"modelv1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cb986e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = GPTModel(GPT_CONFIG_124M)\n",
    "model1.load_state_dict(torch.load(\"modelv1.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7ac92992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca038d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
